{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UupSt5hhWbYW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ricardob/miniconda3/envs/SeqForGraphs/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch\n",
        "from torch.nn import (\n",
        "    BatchNorm1d,\n",
        "    Embedding,\n",
        "    Linear,\n",
        "    ModuleList,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import ZINC, LRGBDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINEConv, global_add_pool\n",
        "import inspect\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, Linear, Sequential\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import (\n",
        "    activation_resolver,\n",
        "    normalization_resolver,\n",
        ")\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "from mamba_ssm import Mamba\n",
        "from torch_geometric.utils import degree, sort_edge_index\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from scipy.sparse.csgraph import floyd_warshall\n",
        "import scipy.sparse.csgraph as csg\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "SPLIT = \"train\"\n",
        "N_SAMPLES_MAX = 2500\n",
        "K = 25\n",
        "SAVE_DIR = f\"../data/precomputed_dist_masks/pept/split={SPLIT}_n={N_SAMPLES_MAX}_k={K}.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomLRGBDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path,\n",
        "        name,\n",
        "        precomputed_masks_path_train=None,\n",
        "        precomputed_masks_path_val=None,\n",
        "        split=\"train\",\n",
        "        transform=None,\n",
        "        n_samples_max=N_SAMPLES_MAX,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tg_dataset = LRGBDataset(path, name=name, split=split, transform=transform)\n",
        "        precomputed_masks_path = precomputed_masks_path_train if split == \"train\" else precomputed_masks_path_val\n",
        "        if precomputed_masks_path is not None:\n",
        "            with open(precomputed_masks_path, \"rb\") as f:\n",
        "                self.precomputed_masks = torch.load(f) # (M, 4) == (graph_idx, node1, node2, dist)\n",
        "        else: \n",
        "            self.precomputed_masks = None\n",
        "        \n",
        "        self.n_samples_max = n_samples_max\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= self.n_samples_max:\n",
        "            raise IndexError\n",
        "        g = self.tg_dataset[idx]\n",
        "        if self.precomputed_masks is not None:\n",
        "            g.dist_mask = self.precomputed_masks[self.precomputed_masks[:,0] == idx]\n",
        "        return g\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples_max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "6mpBj0Rk9uRP"
      },
      "outputs": [],
      "source": [
        "def floyd_warshall(adj_matrix, K):\n",
        "    shortest_paths = csg.floyd_warshall(adj_matrix, directed=False)\n",
        "    k_matrix = np.transpose((np.arange(K+1)[::-1] == shortest_paths[...,None]).astype(int), (2, 0, 1))\n",
        "\n",
        "    return k_matrix \n",
        "\n",
        "train_dataset = CustomLRGBDataset(path = \"/home/ricardob/data/SeqForGraphs/datasets/lrgb\", name=\"Peptides-struct\", split=SPLIT)\n",
        "\n",
        "max_num_modes = 0\n",
        "for i, data in enumerate(train_dataset):\n",
        "    max_num_modes = max(max_num_modes, len(data.x))\n",
        "\n",
        "# Precompute and save dist_masks\n",
        "dist_masks = []\n",
        "for i, data in enumerate(train_dataset):\n",
        "    adj_matrix = to_dense_adj(data.edge_index, max_num_nodes=max_num_modes).squeeze(0).numpy()\n",
        "    k_matrix = floyd_warshall(adj_matrix, K)\n",
        "    dist_masks.append( torch.tensor(k_matrix, dtype=torch.float32) )\n",
        "\n",
        "# Save each dist_mask to disk\n",
        "dist_mask = torch.stack(dist_masks, dim = 0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2500, 26, 434, 434])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dist_mask.shape #(N, K+1, num_nodes, num_nodes) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([39934726, 4])"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compressed represenation\n",
        "compressed_dist_mask = []\n",
        "for k in range(K+1):\n",
        "    k_masks = torch.nonzero(dist_mask[:,k,...] == 1.0) #(n, node_1, node_2)\n",
        "    k_masks = torch.cat([k_masks, torch.tensor([K-k]).repeat( k_masks.shape[0],1)], dim=1) \n",
        "    compressed_dist_mask.append( k_masks  )\n",
        "compressed_dist_mask = torch.cat(compressed_dist_mask, dim=0)\n",
        "compressed_dist_mask.shape # (M,4) == (graph_id, node_1, node_2, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dir from fname\n",
        "os.makedirs(osp.dirname(SAVE_DIR), exist_ok=True)\n",
        "torch.save(compressed_dist_mask, SAVE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
