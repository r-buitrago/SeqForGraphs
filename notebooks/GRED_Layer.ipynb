{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "naxhacg6WaaZ"
      },
      "outputs": [],
      "source": [
        "# !pip install torch_geometric==2.3.1\n",
        "# !pip install mamba-ssm\n",
        "\n",
        "# import torch\n",
        "\n",
        "# !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "# !pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch\n",
        "from torch.nn import (\n",
        "    BatchNorm1d,\n",
        "    Embedding,\n",
        "    Linear,\n",
        "    ModuleList,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINEConv, global_add_pool\n",
        "import inspect\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, Linear, Sequential\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import (\n",
        "    activation_resolver,\n",
        "    normalization_resolver,\n",
        ")\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "from mamba_ssm import Mamba\n",
        "from torch_geometric.utils import degree, sort_edge_index\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "UupSt5hhWbYW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_h, drop_rate=0.):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dim_h = dim_h\n",
        "        self.drop_rate = drop_rate\n",
        "        self.layer_norm = nn.LayerNorm(dim_h)\n",
        "        self.dense1 = nn.Linear(dim_h, dim_h)\n",
        "        self.dropout1 = nn.Dropout(drop_rate)\n",
        "        self.dense2 = nn.Linear(dim_h, dim_h)\n",
        "        self.dropout2 = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, inputs, training=False):\n",
        "        x = self.layer_norm(inputs)\n",
        "        x = self.dense1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x) if training else x\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout2(x) if training else x\n",
        "        return x + inputs"
      ],
      "metadata": {
        "id": "w8yrc_IOWvIE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GREDMamba(torch.nn.Module):\n",
        "    def __init__(self, dim_h):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = MLP(dim_h)\n",
        "        self.self_attn = Mamba(\n",
        "              d_model=dim_h,\n",
        "              d_state=16,\n",
        "              d_conv=4,\n",
        "              expand=1\n",
        "          )\n",
        "\n",
        "    def forward(self, inputs, dist_masks):\n",
        "        # Shape of inputs: (batch_size, num_nodes, dim_h)\n",
        "        # Shape of dist_masks: (batch_size, K+1, num_nodes, num_nodes)\n",
        "        x = torch.swapaxes(dist_masks, 0, 1) @ inputs\n",
        "        # Shape of x: (K+1, batch_size, num_nodes, dim_h)\n",
        "        x = self.mlp(x)\n",
        "        x = x.reshape(x.shape[0] * x.shape[1], x.shape[2], x.shape[3])\n",
        "        # Shape of x: (K+1 * batch_size, num_nodes, dim_h)\n",
        "        x = self.self_attn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uRIh9srtWc5V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "def create_random_graph(batch_size, num_nodes, dim_h, K):\n",
        "  # Create a random graph\n",
        "  G = nx.erdos_renyi_graph(num_nodes, p=0.5)\n",
        "\n",
        "  inputs = np.random.rand(batch_size, num_nodes, dim_h)\n",
        "\n",
        "  dist_masks = np.full((batch_size, K+1, num_nodes, num_nodes), fill_value=np.inf)\n",
        "  np.fill_diagonal(dist_masks[0, 0], 0)\n",
        "\n",
        "  # Replace with floyd_warshall as per GRED implementation\n",
        "  lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
        "  for i in range(num_nodes):\n",
        "      for j in range(num_nodes):\n",
        "          if i in lengths and j in lengths[i] and lengths[i][j] <= K:\n",
        "              dist_masks[0, lengths[i][j], i, j] = 1\n",
        "\n",
        "  dist_masks[dist_masks == np.inf] = 0\n",
        "\n",
        "  return inputs, dist_masks"
      ],
      "metadata": {
        "id": "ToBjkQ-EXZuP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, dist_masks = create_random_graph(1, 10, 16, 3)\n",
        "\n",
        "inputs_tensor = torch.tensor(inputs, dtype=torch.float32).to('cuda')\n",
        "dist_masks_tensor = torch.tensor(dist_masks, dtype=torch.float32).to('cuda')\n",
        "\n",
        "model = GREDMamba(dim_h=inputs.shape[-1]).to('cuda')\n",
        "\n",
        "y = model(inputs_tensor, dist_masks_tensor)\n"
      ],
      "metadata": {
        "id": "ZVlAmnaPXgt3"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}