{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "naxhacg6WaaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b997232-45ab-4509-8165-1085eefd67bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.10/dist-packages (1.2.0.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.7.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.4.99)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
            "Found existing installation: torch_scatter 2.1.2+pt22cu121\n",
            "Uninstalling torch_scatter-2.1.2+pt22cu121:\n",
            "  Successfully uninstalled torch_scatter-2.1.2+pt22cu121\n",
            "Found existing installation: torch_sparse 0.6.18+pt22cu121\n",
            "Uninstalling torch_sparse-0.6.18+pt22cu121:\n",
            "  Successfully uninstalled torch_sparse-0.6.18+pt22cu121\n",
            "Found existing installation: torch_geometric 2.6.0\n",
            "Uninstalling torch_geometric-2.6.0:\n",
            "  Successfully uninstalled torch_geometric-2.6.0\n",
            "Found existing installation: torch_cluster 1.6.3+pt22cu121\n",
            "Uninstalling torch_cluster-1.6.3+pt22cu121:\n",
            "  Successfully uninstalled torch_cluster-1.6.3+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Using cached https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Using cached https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Using cached https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt22cu121\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-oki_2xa7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-oki_2xa7\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 27367b4b0f6f5866d064d2d629375d6d8a0351c4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (3.9.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (3.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (1.25.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.6.0) (4.66.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric==2.6.0) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.6.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.6.0) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.6.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.6.0) (3.3.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.6.0-py3-none-any.whl size=1101012 sha256=209946b4f4e78749355c89167bbaf528775d79b32130d31796546d7e56224d1b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qn9_nxm1/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install torch_geometric==2.3.1\n",
        "!pip install mamba-ssm\n",
        "\n",
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch\n",
        "from torch.nn import (\n",
        "    BatchNorm1d,\n",
        "    Embedding,\n",
        "    Linear,\n",
        "    ModuleList,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINEConv, global_add_pool\n",
        "import inspect\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, Linear, Sequential\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import (\n",
        "    activation_resolver,\n",
        "    normalization_resolver,\n",
        ")\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "from mamba_ssm import Mamba\n",
        "from torch_geometric.utils import degree, sort_edge_index\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from scipy.sparse.csgraph import floyd_warshall\n",
        "import scipy.sparse.csgraph as csg\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UupSt5hhWbYW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_h, drop_rate=0.):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dim_h = dim_h\n",
        "        self.drop_rate = drop_rate\n",
        "        self.layer_norm = nn.LayerNorm(dim_h)\n",
        "        self.dense1 = nn.Linear(dim_h, dim_h)\n",
        "        self.dropout1 = nn.Dropout(drop_rate)\n",
        "        self.dense2 = nn.Linear(dim_h, dim_h)\n",
        "        self.dropout2 = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, inputs, training=False):\n",
        "        x = self.layer_norm(inputs)\n",
        "        x = self.dense1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x) if training else x\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout2(x) if training else x\n",
        "        return x + inputs"
      ],
      "metadata": {
        "id": "w8yrc_IOWvIE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GREDMamba(torch.nn.Module):\n",
        "    def __init__(self, dim_h):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = MLP(dim_h)\n",
        "        self.self_attn = Mamba(\n",
        "              d_model=dim_h,\n",
        "              d_state=16,\n",
        "              d_conv=4,\n",
        "              expand=1\n",
        "          )\n",
        "\n",
        "    def forward(self, inputs, dist_masks):\n",
        "        # Shape of inputs: (batch_size, num_nodes, dim_h)\n",
        "        # Shape of dist_masks: (batch_size, K+1, num_nodes, num_nodes)\n",
        "        x = torch.swapaxes(dist_masks, 0, 1) @ inputs\n",
        "\n",
        "        # Shape of x: (K+1, batch_size, num_nodes, dim_h)\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        # TODO\n",
        "        # x = torch.permute(x, (1, 2, 0, 3)) # Shape of x: (batch_size, num_nodes, K+1, dim_h)\n",
        "\n",
        "        # Shape of x: (K+1 * batch_size, num_nodes, dim_h)\n",
        "        # x = self.self_attn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uRIh9srtWc5V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path, subset = '/scratch/ssd004/scratch/tsepaole/ZINC_full/', False\n",
        "path, subset = '', True\n",
        "\n",
        "train_dataset = ZINC(path, subset=subset, split='train')\n",
        "val_dataset = ZINC(path, subset=subset, split='val')\n",
        "test_dataset = ZINC(path, subset=subset, split='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "K = 4\n",
        "\n",
        "model = GREDMamba(1).to('cuda')\n",
        "\n",
        "for data in train_loader:\n",
        "    inputs, mask = to_dense_batch(data.x, data.batch) # Shape of inputs: (batch_size, num_nodes, dim_h)\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    mask = mask.float().to('cuda')\n",
        "\n",
        "    # Create distance matrix using floyd_warshall\n",
        "    adj_matrix = to_dense_adj(data.edge_index, batch=data.batch, max_num_nodes=inputs.shape[1])\n",
        "\n",
        "    # Compute the shortest paths using Floyd-Warshall\n",
        "    shortest_paths = [csg.floyd_warshall(i, directed=False) for i in adj_matrix.numpy()]\n",
        "    shortest_paths = [np.transpose((np.arange(K+1)[::-1] == i[...,None]).astype(int), (2, 0, 1)) for i in shortest_paths]\n",
        "\n",
        "    # Shape of dist_masks: (batch_size, K+1, num_nodes, num_nodes)\n",
        "    dist_mask = torch.tensor(np.array(shortest_paths), dtype=torch.float32).to('cuda')\n",
        "\n",
        "    out = model(inputs, dist_mask)"
      ],
      "metadata": {
        "id": "tWLc69GBUdy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNthcI3mOJL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}